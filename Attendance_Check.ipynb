{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attendance_Check.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArpitaChatterjee/FaceRecognition-Attendance/blob/main/Attendance_Check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vnzc-o_MLqK",
        "outputId": "1b0e57e6-dffc-4561-94fa-ea393723b8ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip  install dlib==19.18.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dlib==19.18.0 in /usr/local/lib/python3.6/dist-packages (19.18.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi9c0c_2J5fx",
        "outputId": "ae5717b3-ee91-4d02-e01d-5bd65d1a9e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install face_recognition\n",
        "import dlib\n",
        "import face_recognition"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (19.18.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face_recognition) (1.18.5)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n",
            "\u001b[K     |████████████████████████████████| 100.2MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.0.0)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566173 sha256=f6e72156adb8fb0a5e275924e544799bdfff1fc65d95c6ada14b27266220e8ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wQd7muAL8DF",
        "outputId": "7b03053d-928b-420a-bbc1-3feebdd582c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install opencv-python\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-0TDFPDM3Q2",
        "outputId": "1b1c4260-6791-4e70-b198-e68c68367901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path = '/content/drive/My Drive/Colab Notebooks/Face Recognition/ImagesAttendance'\n",
        "images=[]\n",
        "classNames=[]\n",
        "mylist= os.listdir(path)\n",
        "print(mylist)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Bill Gates.jpg', 'Jack Ma.jpg', 'Elon Musk.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Lhe7HIO1g-",
        "outputId": "afc89ab1-bd67-406b-ea3e-b2c650f70f33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for cl in mylist:\n",
        "  curImg = cv2.imread(f'{path}/{cl}')\n",
        "  images.append(curImg)\n",
        "  classNames.append(os.path.splitext(cl)[0])\n",
        "print(classNames)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Bill Gates', 'Jack Ma', 'Elon Musk']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5j84NrUPkDB",
        "outputId": "5d8a1af2-f7fe-451f-c898-9305294c6c60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def findEncodings(images):\n",
        "  encodeList =[]\n",
        "  for img in images:\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    encode = face_recognition.face_encodings(img)[0]\n",
        "    encodeList.append(encode)\n",
        "  return encodeList\n",
        "\n",
        "encodelistknown = findEncodings(images)\n",
        "print(len(encodelistknown))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRsP5BsIQ9Cy",
        "outputId": "37c52003-f9ef-4aaa-cc81-0a3613aef78a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('encoding Complete')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoding Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeRzfTf-vhrm"
      },
      "source": [
        "from datetime import datetime"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DalnC25iuyov"
      },
      "source": [
        "#TO Mark Attendence\n",
        "def markAttendence(name):\n",
        "  with open('/content/drive/My Drive/Colab Notebooks/Face Recognition/Attendance.csv', 'r+') as f:\n",
        "    mydatalist = f.readlines()\n",
        "    namelist=[]\n",
        "    for line in mydatalist:\n",
        "      entry = line.split(',')\n",
        "      namelist.append(entry[0])\n",
        "    if name not in namelist:\n",
        "      now = datetime.now()\n",
        "      dtstr = now.strftime('%H:%M:%S')\n",
        "      f.writelines(f'\\n{name}, {dtstr}')\n",
        "\n",
        "markAttendence('Elon')\n",
        "markAttendence('Bill Gates')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6rC5YxpYe1J"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrVLbQbJYe0U",
        "outputId": "bd8d0070-21ec-47eb-b476-a8fc119080f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  cap = take_photo()\n",
        "  print('Saved to {}'.format(cap))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  #display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saved to photo.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo560y82tM_o",
        "outputId": "a3a79fd9-5d7f-4011-d9c7-74a7939276e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmiFHPgTRGQb",
        "outputId": "f66f73e9-6b50-42dd-efa7-639213d090af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "#cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "  sucess, img = cap.read()\n",
        "  imgS = cv2.resize(img, (0,0), None, 0.25, 0.25)\n",
        "  imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  facesCurFrame = face_recognition.face_locations(imgS)\n",
        "  encodesCurFrame = face_recognition.face_encodings(imgS, facesCurFrame)\n",
        "\n",
        "  for encodeFace, faceloc in zip(encodesCurFrame, facesCurFrame):\n",
        "    matches = face_recognition.compare_faces(encodelistknown, encodeFace)\n",
        "    faceDis = face_recognition.face_distance(encodelistknown, encodeFace)\n",
        "    print(faceDis)\n",
        "    matchIndex = np.argmin(faceDis)\n",
        "\n",
        "    if matches[matchIndex]:\n",
        "      name=classNames[matchIndex].upper()\n",
        "      print(name)\n",
        "      y1,y1,y2,x1 =faceloc\n",
        "      y1,y1,y2,x1 = y1*4, y1*4, y2*4, x1*4\n",
        "      cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "      cv2.rectangle(img, (x1,y2-35), (x2,y2), (0,255,0), cv2.FILLED)\n",
        "      cv2.putText(img, name, (x1+6, y2-6), cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,255), 2)\n",
        "\n",
        "      #when the face matches it will call the attendence fn and put the naem of the person detected by this \n",
        "      #mth directly to the Attendence.csv file that was created in order to mark attendence directly from \n",
        "      #face-recognition\n",
        "      markAttendence(name)\n",
        "\n",
        "cv2.imshow('webcam', img)\n",
        "cv2.waitKey(0)\n",
        "\n",
        "      \n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-020e3d3a4a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0msucess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mimgS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mimgS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
          ]
        }
      ]
    }
  ]
}