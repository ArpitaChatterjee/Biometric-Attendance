{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attendance_Check.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11hYQ1PTM3GLNwpzLJl84fSiQqts9FyHM",
      "authorship_tag": "ABX9TyMKWjHUWb9avGVJdguG7+Mh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArpitaChatterjee/Biometric-Attendance/blob/main/Attendance_Check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vnzc-o_MLqK",
        "outputId": "afee49d9-fa4f-483b-ee97-528d818e387d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip  install dlib==19.18.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dlib==19.18.0 in /usr/local/lib/python3.6/dist-packages (19.18.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi9c0c_2J5fx",
        "outputId": "061c216c-7548-4e24-9b46-249ae4fcc6e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install face_recognition\n",
        "import dlib\n",
        "import face_recognition"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.0.0)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (19.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face_recognition) (1.18.5)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n",
            "\u001b[K     |████████████████████████████████| 100.2MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.1.2)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566173 sha256=00c73e63d682ef0ba2b6d92b353b580f693bee65d3e2cde7e5effee01e54fda0\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wQd7muAL8DF",
        "outputId": "4b357388-894d-4274-f72a-cafedeef526d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install opencv-python\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-0TDFPDM3Q2",
        "outputId": "aeb3d884-b7e4-47f6-ed88-36131e2c2433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path = '/content/drive/My Drive/Colab Notebooks/Face Recognition/ImagesAttendance'\n",
        "images=[]\n",
        "classNames=[]\n",
        "mylist= os.listdir(path)\n",
        "print(mylist)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Bill Gates.jpg', 'Jack Ma.jpg', 'Elon Musk.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Lhe7HIO1g-",
        "outputId": "4e92003e-e61d-425e-9372-ae4fef3fc460",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for cl in mylist:\n",
        "  curImg = cv2.imread(f'{path}/{cl}')\n",
        "  images.append(curImg)\n",
        "  classNames.append(os.path.splitext(cl)[0])\n",
        "print(classNames)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Bill Gates', 'Jack Ma', 'Elon Musk']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5j84NrUPkDB",
        "outputId": "1bc3f893-a545-4df4-b14d-14b269c8429b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def findEncodings(images):\n",
        "  encodeList =[]\n",
        "  for img in images:\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    encode = face_recognition.face_encodings(img)[0]\n",
        "    encodeList.append(encode)\n",
        "  return encodeList\n",
        "\n",
        "encodelistknown = findEncodings(images)\n",
        "print(len(encodelistknown))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRsP5BsIQ9Cy",
        "outputId": "dda18236-870f-4795-ac42-9b4ff88ccfd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('encoding Complete')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoding Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeRzfTf-vhrm"
      },
      "source": [
        "from datetime import datetime"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DalnC25iuyov"
      },
      "source": [
        "#TO Mark Attendence\n",
        "def markAttendence(name):\n",
        "  with open('/content/drive/My Drive/Colab Notebooks/Face Recognition/Attendance.csv', 'r+') as f:\n",
        "    mydatalist = f.readlines()\n",
        "    namelist=[]\n",
        "    for line in mydatalist:\n",
        "      entry = line.split(',')\n",
        "      namelist.append(entry[0])\n",
        "    if name not in namelist:\n",
        "      now = datetime.now()\n",
        "      dtstr = now.strftime('%H:%M:%S')\n",
        "      f.writelines(f'\\n{name}, {dtstr}')\n",
        "\n",
        "markAttendence('Elon')\n",
        "markAttendence('Bill Gates')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6rC5YxpYe1J"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrVLbQbJYe0U",
        "outputId": "cef233cc-c829-42e5-c1ef-c45636b12819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  #display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saved to photo.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmiFHPgTRGQb"
      },
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "  sucess, img = cap.read()\n",
        "  imgS = cv2.resize(img, (0,0), None, 0.25, 0.25)\n",
        "  imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  facesCurFrame = face_recognition.face_locations(imgS)\n",
        "  encodesCurFrame = face_recognition.face_encodings(imgS, facesCurFrame)\n",
        "\n",
        "  for encodeFace, faceloc in zip(encodesCurFrame, facesCurFrame):\n",
        "    matches = face_recognition.compare_faces(encodelistknown, encodeFace)\n",
        "    faceDis = face_recognition.face_distance(encodelistknown, encodeFace)\n",
        "    print(faceDis)\n",
        "    matchIndex = np.argmin(faceDis)\n",
        "\n",
        "    if matches[matchIndex]:\n",
        "      name=classNames[matchIndex].upper()\n",
        "      print(name)\n",
        "      y1,y1,y2,x1 =faceloc\n",
        "      y1,y1,y2,x1 = y1*4, y1*4, y2*4, x1*4\n",
        "      cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "      cv2.rectangle(img, (x1,y2-35), (x2,y2), (0,255,0), cv2.FILLED)\n",
        "      cv2.putText(img, name, (x1+6, y2-6), cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,255), 2)\n",
        "\n",
        "      #when the face matches it will call the attendence fn and put the naem of the person detected by this \n",
        "      #mth directly to the Attendence.csv file that was created in order to mark attendence directly from \n",
        "      #face-recognition\n",
        "      markAttendence(name)\n",
        "\n",
        "cv2.imshow('webcam', img)\n",
        "cv2.waitKey(0)\n",
        "\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
